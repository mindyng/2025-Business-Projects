* [Anthropic Research Team: Societal Impact, Alignment, Interpretability](https://www.anthropic.com/research)
* [Center for AI Safety](https://www.safe.ai/careers)
* [Future of Life Institute](https://futureoflife.org/)
* [Alignment Research Center](https://www.alignment.org/)
* Practically Seeing the Reason Why by Jail Breaking with [Gray Swan Arena](https://app.grayswan.ai/arena)
* [Machine Intelligence Research Institute](https://intelligence.org/get-involved/)
* [How to Get Started in Alignment](https://www.alignmentforum.org/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide)
  * [Author with LI connections to reach out to](https://www.linkedin.com/in/wentworthjohn/)
* [How to Get Into Independent Research on Alignment/Agency](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency)
* [Special Competitive Studies Project](https://www.scsp.ai/)
* [80,000 Hours](https://80000hours.org)
  * https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/
* [Open Philanthropy](https://www.openphilanthropy.org/)
* [Center for Humane Technology](https://www.humanetech.com/)
* [Foresight Institute](https://foresight.org/technologies/secure-ai/)
