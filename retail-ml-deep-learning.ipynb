{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f256293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T22:36:19.510818Z",
     "iopub.status.busy": "2025-01-15T22:36:19.510387Z",
     "iopub.status.idle": "2025-01-15T22:36:38.558259Z",
     "shell.execute_reply": "2025-01-15T22:36:38.556512Z"
    },
    "papermill": {
     "duration": 19.054367,
     "end_time": "2025-01-15T22:36:38.560618",
     "exception": false,
     "start_time": "2025-01-15T22:36:19.506251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.layers import Input, Embedding, concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82cf6190",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-15T22:36:38.567352Z",
     "iopub.status.busy": "2025-01-15T22:36:38.566447Z",
     "iopub.status.idle": "2025-01-15T22:36:49.927731Z",
     "shell.execute_reply": "2025-01-15T22:36:49.926565Z"
    },
    "papermill": {
     "duration": 11.366567,
     "end_time": "2025-01-15T22:36:49.929552",
     "exception": false,
     "start_time": "2025-01-15T22:36:38.562985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Recommendation System...\n",
      "Epoch 1/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.4888 - loss: 0.6936 - val_accuracy: 0.4930 - val_loss: 0.6929\n",
      "Epoch 2/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6505 - loss: 0.6838 - val_accuracy: 0.5060 - val_loss: 0.7023\n",
      "Epoch 3/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6803 - loss: 0.6128 - val_accuracy: 0.5080 - val_loss: 0.7401\n",
      "Epoch 4/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7558 - loss: 0.5329 - val_accuracy: 0.5080 - val_loss: 0.8232\n",
      "Epoch 5/5\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7901 - loss: 0.4764 - val_accuracy: 0.5080 - val_loss: 0.9314\n",
      "\n",
      "Training Churn Prediction Model...\n",
      "Epoch 1/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.6235 - loss: 0.6424 - val_accuracy: 0.8400 - val_loss: 0.4991\n",
      "Epoch 2/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8306 - loss: 0.4904 - val_accuracy: 0.9200 - val_loss: 0.3265\n",
      "Epoch 3/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8614 - loss: 0.3603 - val_accuracy: 0.9350 - val_loss: 0.2126\n",
      "Epoch 4/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9123 - loss: 0.2626 - val_accuracy: 0.9500 - val_loss: 0.1560\n",
      "Epoch 5/5\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9338 - loss: 0.2134 - val_accuracy: 0.9800 - val_loss: 0.1191\n",
      "\n",
      "Models trained successfully!\n",
      "\n",
      "Example Recommendations for user 1:\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "[290 393 487 431 277]\n",
      "\n",
      "Example Churn Risk for a customer sequence:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "Churn Risk: 8.49%\n"
     ]
    }
   ],
   "source": [
    "# 1. Product Recommendation System using Neural Collaborative Filtering\n",
    "def build_ncf_model(num_users, num_products, embedding_size=50):\n",
    "    \"\"\"\n",
    "    Neural Collaborative Filtering model for product recommendations\n",
    "    \"\"\"\n",
    "    # User input\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    user_embedding = Embedding(num_users, embedding_size, name='user_embedding')(user_input)\n",
    "    user_vec = Flatten()(user_embedding)\n",
    "\n",
    "    # Product input\n",
    "    product_input = Input(shape=(1,), name='product_input')\n",
    "    product_embedding = Embedding(num_products, embedding_size, name='product_embedding')(product_input)\n",
    "    product_vec = Flatten()(product_embedding)\n",
    "\n",
    "    # Merge layers\n",
    "    concat = concatenate([user_vec, product_vec])\n",
    "    dense1 = Dense(128, activation='relu')(concat)\n",
    "    dropout1 = Dropout(0.2)(dense1)\n",
    "    dense2 = Dense(64, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.2)(dense2)\n",
    "    output = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "    model = Model(inputs=[user_input, product_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 2. Customer Churn Prediction using LSTM\n",
    "def build_churn_prediction_model(sequence_length, num_features):\n",
    "    \"\"\"\n",
    "    LSTM model for predicting customer churn based on purchase history\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(sequence_length, num_features)),  # Explicit Input layer\n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 3. Product Image Classification using CNN\n",
    "def build_product_classification_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    CNN model for classifying product images by category\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape), \n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4. Sales Forecasting using LSTM\n",
    "def build_sales_forecast_model(sequence_length, num_features):\n",
    "    \"\"\"\n",
    "    LSTM model for forecasting sales\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(sequence_length, num_features)), \n",
    "        LSTM(50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dense(25, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Example data generation and preprocessing functions\n",
    "def generate_sample_data():\n",
    "    \"\"\"\n",
    "    Generate sample data for testing models\n",
    "    \"\"\"\n",
    "    # User-Product Interaction Data\n",
    "    n_users = 1000\n",
    "    n_products = 500\n",
    "    n_interactions = 5000\n",
    "    \n",
    "    interactions = {\n",
    "        'user_id': np.random.randint(0, n_users, n_interactions),\n",
    "        'product_id': np.random.randint(0, n_products, n_interactions),\n",
    "        'rating': np.random.uniform(1, 5, n_interactions),\n",
    "        'timestamp': pd.date_range(start='2023-01-01', periods=n_interactions, freq='h')\n",
    "    }\n",
    "    \n",
    "    # Customer Purchase History Data\n",
    "    n_customers = 1000\n",
    "    sequence_length = 10\n",
    "    \n",
    "    purchase_history = {\n",
    "        'customer_id': np.repeat(range(n_customers), sequence_length),\n",
    "        'purchase_amount': np.random.normal(100, 30, n_customers * sequence_length),\n",
    "        'days_since_last_purchase': np.random.randint(1, 100, n_customers * sequence_length),\n",
    "        'products_bought': np.random.randint(1, 10, n_customers * sequence_length)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(interactions), pd.DataFrame(purchase_history)\n",
    "\n",
    "def preprocess_recommendation_data(interactions_df):\n",
    "    \"\"\"\n",
    "    Preprocess data for recommendation system\n",
    "    \"\"\"\n",
    "    # Encode users and products\n",
    "    user_encoder = LabelEncoder()\n",
    "    product_encoder = LabelEncoder()\n",
    "    \n",
    "    interactions_df['user_encoded'] = user_encoder.fit_transform(interactions_df['user_id']) # IS THIS RIGHT??\n",
    "    interactions_df['product_encoded'] = product_encoder.fit_transform(interactions_df['product_id'])\n",
    "    \n",
    "    # Create training data\n",
    "    X_user = interactions_df['user_encoded'].values\n",
    "    X_product = interactions_df['product_encoded'].values\n",
    "    y = (interactions_df['rating'] > 3).astype(int).values  # Convert to binary feedback\n",
    "    \n",
    "    return (X_user, X_product), y, user_encoder, product_encoder\n",
    "\n",
    "def preprocess_churn_data(purchase_history_df):\n",
    "    \"\"\"\n",
    "    Preprocess data for churn prediction\n",
    "    \"\"\"\n",
    "    # Create sequences of customer behavior\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for customer in purchase_history_df['customer_id'].unique():\n",
    "        customer_data = purchase_history_df[purchase_history_df['customer_id'] == customer]\n",
    "        \n",
    "        if len(customer_data) >= 10:  # Ensure enough history\n",
    "            sequence = customer_data[['purchase_amount', 'days_since_last_purchase', 'products_bought']].values\n",
    "            sequences.append(sequence[:10])  # Take last 10 purchases\n",
    "            \n",
    "            # Define churn based on days since last purchase\n",
    "            churn = 1 if customer_data['days_since_last_purchase'].iloc[-1] > 60 else 0\n",
    "            labels.append(churn)\n",
    "    \n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate sample data\n",
    "    interactions_df, purchase_history_df = generate_sample_data()\n",
    "    \n",
    "    # 1. Recommendation System\n",
    "    print(\"Training Recommendation System...\")\n",
    "    (X_user, X_product), y, user_encoder, product_encoder = preprocess_recommendation_data(interactions_df)\n",
    "    \n",
    "    num_users = len(np.unique(X_user))\n",
    "    num_products = len(np.unique(X_product))\n",
    "    \n",
    "    ncf_model = build_ncf_model(num_users, num_products)\n",
    "    ncf_model.fit(\n",
    "        [X_user, X_product], \n",
    "        y,\n",
    "        batch_size=64,\n",
    "        epochs=5,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    # 2. Churn Prediction\n",
    "    print(\"\\nTraining Churn Prediction Model...\")\n",
    "    X_sequences, y_churn = preprocess_churn_data(purchase_history_df)\n",
    "    \n",
    "    churn_model = build_churn_prediction_model(\n",
    "        sequence_length=10,\n",
    "        num_features=X_sequences.shape[2]\n",
    "    )\n",
    "    churn_model.fit(\n",
    "        X_sequences,\n",
    "        y_churn,\n",
    "        batch_size=32,\n",
    "        epochs=5,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    print(\"\\nModels trained successfully!\")\n",
    "\n",
    "    # Example business applications\n",
    "    def get_product_recommendations(user_id, top_n=5):\n",
    "        \"\"\"\n",
    "        Get product recommendations for a user\n",
    "        \"\"\"\n",
    "        user_encoded = user_encoder.transform([user_id])\n",
    "        all_products = np.arange(num_products)\n",
    "        \n",
    "        predictions = ncf_model.predict([\n",
    "            np.repeat(user_encoded, num_products),\n",
    "            all_products\n",
    "        ])\n",
    "        \n",
    "        top_product_indices = np.argsort(predictions.flatten())[-top_n:]\n",
    "        return product_encoder.inverse_transform(top_product_indices)\n",
    "\n",
    "    def predict_customer_churn_risk(customer_sequence):\n",
    "        \"\"\"\n",
    "        Predict churn risk for a customer\n",
    "        \"\"\"\n",
    "        sequence = np.expand_dims(customer_sequence, axis=0)\n",
    "        return churn_model.predict(sequence)[0][0]\n",
    "\n",
    "    # Example usage\n",
    "    print(\"\\nExample Recommendations for user 1:\")\n",
    "    print(get_product_recommendations(0))\n",
    "    \n",
    "    print(\"\\nExample Churn Risk for a customer sequence:\")\n",
    "    print(f\"Churn Risk: {predict_customer_churn_risk(X_sequences[0]):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de92f1",
   "metadata": {
    "papermill": {
     "duration": 0.006175,
     "end_time": "2025-01-15T22:36:49.944153",
     "exception": false,
     "start_time": "2025-01-15T22:36:49.937978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 36.660109,
   "end_time": "2025-01-15T22:36:52.807042",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-15T22:36:16.146933",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
