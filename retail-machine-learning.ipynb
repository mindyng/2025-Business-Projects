{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b4e442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T23:27:39.721445Z",
     "iopub.status.busy": "2025-01-15T23:27:39.721121Z",
     "iopub.status.idle": "2025-01-15T23:27:42.362218Z",
     "shell.execute_reply": "2025-01-15T23:27:42.361285Z"
    },
    "papermill": {
     "duration": 2.647089,
     "end_time": "2025-01-15T23:27:42.364106",
     "exception": false,
     "start_time": "2025-01-15T23:27:39.717017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28e325f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T23:27:42.370825Z",
     "iopub.status.busy": "2025-01-15T23:27:42.370349Z",
     "iopub.status.idle": "2025-01-15T23:27:42.482335Z",
     "shell.execute_reply": "2025-01-15T23:27:42.481347Z"
    },
    "papermill": {
     "duration": 0.117074,
     "end_time": "2025-01-15T23:27:42.484270",
     "exception": false,
     "start_time": "2025-01-15T23:27:42.367196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers = pd.read_csv('/kaggle/input/e-commerce-event-data/seph_customers.csv')\n",
    "transactions = pd.read_csv('/kaggle/input/e-commerce-event-data/seph_transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdeabef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T23:27:42.490519Z",
     "iopub.status.busy": "2025-01-15T23:27:42.490150Z",
     "iopub.status.idle": "2025-01-15T23:27:42.509423Z",
     "shell.execute_reply": "2025-01-15T23:27:42.508339Z"
    },
    "papermill": {
     "duration": 0.024378,
     "end_time": "2025-01-15T23:27:42.511323",
     "exception": false,
     "start_time": "2025-01-15T23:27:42.486945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get rid of extraneous columns\n",
    "customers.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "transactions.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345b2ed1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-15T23:27:42.517516Z",
     "iopub.status.busy": "2025-01-15T23:27:42.517204Z",
     "iopub.status.idle": "2025-01-15T23:27:45.597916Z",
     "shell.execute_reply": "2025-01-15T23:27:45.596673Z"
    },
    "papermill": {
     "duration": 3.085728,
     "end_time": "2025-01-15T23:27:45.599686",
     "exception": false,
     "start_time": "2025-01-15T23:27:42.513958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>product_level_1</th>\n",
       "      <th>product_level_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_357806</td>\n",
       "      <td>CUST_000000</td>\n",
       "      <td>2024-12-10</td>\n",
       "      <td>Skincare</td>\n",
       "      <td>8</td>\n",
       "      <td>48.527557</td>\n",
       "      <td>388.220452</td>\n",
       "      <td>Cleansers</td>\n",
       "      <td>Oil Cleanser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_890844</td>\n",
       "      <td>CUST_000000</td>\n",
       "      <td>2024-01-20</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>4</td>\n",
       "      <td>61.553292</td>\n",
       "      <td>246.213169</td>\n",
       "      <td>Perfume</td>\n",
       "      <td>Eau de Toilette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_862276</td>\n",
       "      <td>CUST_000000</td>\n",
       "      <td>2024-08-16</td>\n",
       "      <td>Haircare</td>\n",
       "      <td>8</td>\n",
       "      <td>37.358774</td>\n",
       "      <td>298.870189</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>Oily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_110665</td>\n",
       "      <td>CUST_000000</td>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>Skincare</td>\n",
       "      <td>4</td>\n",
       "      <td>43.358752</td>\n",
       "      <td>173.435010</td>\n",
       "      <td>Cleansers</td>\n",
       "      <td>Micellar Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_379781</td>\n",
       "      <td>CUST_000000</td>\n",
       "      <td>2024-08-22</td>\n",
       "      <td>Haircare</td>\n",
       "      <td>4</td>\n",
       "      <td>23.248625</td>\n",
       "      <td>92.994500</td>\n",
       "      <td>Styling</td>\n",
       "      <td>Hair Oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26381</th>\n",
       "      <td>TXN_778123</td>\n",
       "      <td>CUST_000999</td>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>Skincare</td>\n",
       "      <td>4</td>\n",
       "      <td>52.116852</td>\n",
       "      <td>208.467409</td>\n",
       "      <td>Cleansers</td>\n",
       "      <td>Foam Cleanser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26382</th>\n",
       "      <td>TXN_542317</td>\n",
       "      <td>CUST_000999</td>\n",
       "      <td>2024-06-21</td>\n",
       "      <td>Haircare</td>\n",
       "      <td>6</td>\n",
       "      <td>49.535322</td>\n",
       "      <td>297.211932</td>\n",
       "      <td>Conditioner</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26383</th>\n",
       "      <td>TXN_054181</td>\n",
       "      <td>CUST_000999</td>\n",
       "      <td>2024-07-06</td>\n",
       "      <td>Haircare</td>\n",
       "      <td>3</td>\n",
       "      <td>40.263499</td>\n",
       "      <td>120.790496</td>\n",
       "      <td>Styling</td>\n",
       "      <td>Hair Spray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26384</th>\n",
       "      <td>TXN_717607</td>\n",
       "      <td>CUST_000999</td>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>Makeup</td>\n",
       "      <td>4</td>\n",
       "      <td>34.409350</td>\n",
       "      <td>137.637400</td>\n",
       "      <td>Eyes</td>\n",
       "      <td>Eyeliner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26385</th>\n",
       "      <td>TXN_475915</td>\n",
       "      <td>CUST_000999</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Haircare</td>\n",
       "      <td>2</td>\n",
       "      <td>37.313109</td>\n",
       "      <td>74.626219</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>Regular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26386 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      transaction_id  customer_id        date   category  quantity  \\\n",
       "0         TXN_357806  CUST_000000  2024-12-10   Skincare         8   \n",
       "1         TXN_890844  CUST_000000  2024-01-20  Fragrance         4   \n",
       "2         TXN_862276  CUST_000000  2024-08-16   Haircare         8   \n",
       "3         TXN_110665  CUST_000000  2024-09-20   Skincare         4   \n",
       "4         TXN_379781  CUST_000000  2024-08-22   Haircare         4   \n",
       "...              ...          ...         ...        ...       ...   \n",
       "26381     TXN_778123  CUST_000999  2024-02-26   Skincare         4   \n",
       "26382     TXN_542317  CUST_000999  2024-06-21   Haircare         6   \n",
       "26383     TXN_054181  CUST_000999  2024-07-06   Haircare         3   \n",
       "26384     TXN_717607  CUST_000999  2024-04-04     Makeup         4   \n",
       "26385     TXN_475915  CUST_000999  2025-01-01   Haircare         2   \n",
       "\n",
       "       unit_price  total_amount product_level_1  product_level_2  \n",
       "0       48.527557    388.220452       Cleansers     Oil Cleanser  \n",
       "1       61.553292    246.213169         Perfume  Eau de Toilette  \n",
       "2       37.358774    298.870189         Shampoo             Oily  \n",
       "3       43.358752    173.435010       Cleansers   Micellar Water  \n",
       "4       23.248625     92.994500         Styling         Hair Oil  \n",
       "...           ...           ...             ...              ...  \n",
       "26381   52.116852    208.467409       Cleansers    Foam Cleanser  \n",
       "26382   49.535322    297.211932     Conditioner          Regular  \n",
       "26383   40.263499    120.790496         Styling       Hair Spray  \n",
       "26384   34.409350    137.637400            Eyes         Eyeliner  \n",
       "26385   37.313109     74.626219         Shampoo          Regular  \n",
       "\n",
       "[26386 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include product data to transactions\n",
    "product_hierarchy = {\n",
    "    'Skincare': {\n",
    "        'Moisturizers': ['Day Cream', 'Night Cream', 'Eye Cream'],\n",
    "        'Cleansers': ['Foam Cleanser', 'Oil Cleanser', 'Micellar Water'],\n",
    "        'Treatments': ['Vitamin C Serum', 'Retinol', 'Hyaluronic Acid']\n",
    "    },\n",
    "    'Makeup': {\n",
    "        'Face': ['Foundation', 'Concealer', 'Blush', 'Bronzer'],\n",
    "        'Eyes': ['Mascara', 'Eyeshadow', 'Eyeliner'],\n",
    "        'Lips': ['Lipstick', 'Lip Gloss', 'Lip Liner']\n",
    "    },\n",
    "    'Fragrance': {\n",
    "        'Perfume': ['Eau de Parfum', 'Eau de Toilette'],\n",
    "        'Sets': ['Gift Set', 'Travel Set']\n",
    "    },\n",
    "    'Haircare': {\n",
    "        'Shampoo': ['Regular', 'Dry', 'Oily'],\n",
    "        'Conditioner': ['Regular', 'Deep', 'Leave-in'],\n",
    "        'Styling': ['Hair Oil', 'Heat Protectant', 'Hair Spray']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add product level 1 and product level 2 columns\n",
    "def assign_product_details(category):\n",
    "    \"\"\"Assign product-level details based on category.\"\"\"\n",
    "    # Select a random product level 1 (subcategory)\n",
    "    product_level_1 = random.choice(list(product_hierarchy[category].keys()))\n",
    "    # Select a random product level 2 (actual product) within the chosen subcategory\n",
    "    product_level_2 = random.choice(product_hierarchy[category][product_level_1])\n",
    "    return product_level_1, product_level_2\n",
    "\n",
    "# Apply the function to the transactions dataframe\n",
    "transactions[['product_level_1', 'product_level_2']] = transactions['category'].apply(\n",
    "    lambda x: pd.Series(assign_product_details(x))\n",
    ")\n",
    "\n",
    "# Display the updated transactions dataframe\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efcb35d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-15T23:27:45.606826Z",
     "iopub.status.busy": "2025-01-15T23:27:45.606468Z",
     "iopub.status.idle": "2025-01-15T23:27:46.251422Z",
     "shell.execute_reply": "2025-01-15T23:27:46.250116Z"
    },
    "papermill": {
     "duration": 0.650857,
     "end_time": "2025-01-15T23:27:46.253572",
     "exception": false,
     "start_time": "2025-01-15T23:27:45.602715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LTV Analysis:\n",
      "                    feature  importance\n",
      "2        purchase_frequency    0.834208\n",
      "3     avg_transaction_value    0.165573\n",
      "1             loyalty_score    0.000092\n",
      "6  days_since_last_purchase    0.000070\n",
      "0                       age    0.000057\n",
      "4        category_diversity    0.000000\n",
      "5     premium_product_ratio    0.000000\n",
      "\n",
      "Churn Risk Analysis:\n",
      "                 feature  importance\n",
      "3  avg_transaction_value    0.348021\n",
      "1          loyalty_score    0.306373\n",
      "2     purchase_frequency    0.242280\n",
      "0                    age    0.103179\n",
      "4     category_diversity    0.000147\n",
      "5  premium_product_ratio    0.000000\n",
      "\n",
      "Customer Segments:\n",
      "customer_segment\n",
      "Regular Customers    848\n",
      "Lost Customers        83\n",
      "Champions             51\n",
      "Loyal Customers       18\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class CustomerAnalyticsSuite:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the analytics suite with required models and transformers.\"\"\"\n",
    "        self.scaler = StandardScaler()\n",
    "        self.ltv_model = GradientBoostingRegressor(random_state=42)\n",
    "        self.churn_model = RandomForestClassifier(random_state=42)\n",
    "        \n",
    "    def prepare_customer_features(self, \n",
    "                                customers: pd.DataFrame, \n",
    "                                transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create feature-rich customer profiles from raw data.\"\"\"\n",
    "\n",
    "        # Calculate customer engagement metrics\n",
    "        engagement_features = self._calculate_engagement_metrics(customers, transactions)\n",
    "        \n",
    "        # Calculate transaction-based features\n",
    "        transaction_features = self._calculate_transaction_features(transactions)\n",
    "        \n",
    "        # Merge all features\n",
    "        features = customers.merge(\n",
    "            engagement_features,\n",
    "            on='customer_id',\n",
    "            how='left'\n",
    "        ).merge(\n",
    "            transaction_features,\n",
    "            on='customer_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill missing values for customers with no transactions\n",
    "        features = features.fillna({\n",
    "            'avg_transaction_value': 0,\n",
    "            'purchase_frequency': 0,\n",
    "            'days_since_last_purchase': 365,  # Assume max days\n",
    "            'category_diversity': 0,\n",
    "            'premium_product_ratio': 0\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _calculate_engagement_metrics(self, \n",
    "                                   customers: pd.DataFrame,\n",
    "                                   transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate customer engagement metrics.\"\"\"\n",
    "        \n",
    "        # Category diversity\n",
    "        category_diversity = (\n",
    "            transactions.groupby('customer_id')['category']\n",
    "            .agg(lambda x: len(set(x)))\n",
    "            .reset_index()\n",
    "            .rename(columns={'category': 'category_diversity'})\n",
    "        )\n",
    "        \n",
    "        # Premium product ratio\n",
    "        premium_products = (\n",
    "            transactions.groupby('customer_id')['product_level_2']\n",
    "            .agg(lambda x: np.mean(np.array(x) == 'Luxury'))\n",
    "            .reset_index()\n",
    "            .rename(columns={'product_level_2': 'premium_product_ratio'})\n",
    "        )\n",
    "        \n",
    "        # Merge engagement metrics\n",
    "        engagement_metrics = category_diversity.merge(\n",
    "            premium_products,\n",
    "            on='customer_id',\n",
    "            how='outer'\n",
    "        )\n",
    "        \n",
    "        return engagement_metrics\n",
    "    \n",
    "    def _calculate_transaction_features(self, transactions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate customer-level transaction features.\"\"\"\n",
    "        \n",
    "        # Basic transaction metrics\n",
    "        transaction_metrics = transactions.groupby('customer_id').agg({\n",
    "            'transaction_id': 'count',\n",
    "            'total_amount': ['mean', 'sum', 'std'],\n",
    "            'date': ['min', 'max']\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        transaction_metrics.columns = [\n",
    "            'customer_id', 'transaction_count', 'avg_transaction_value',\n",
    "            'total_spend', 'spend_std', 'first_purchase_date', 'last_purchase_date'\n",
    "        ]\n",
    "        \n",
    "        # Calculate days between first and last purchase\n",
    "        transaction_metrics['customer_age_days'] = (\n",
    "            pd.to_datetime(transaction_metrics['last_purchase_date']) -\n",
    "            pd.to_datetime(transaction_metrics['first_purchase_date'])\n",
    "        ).dt.days\n",
    "        \n",
    "        # Calculate purchase frequency (transactions per month)\n",
    "        transaction_metrics['purchase_frequency'] = (\n",
    "            transaction_metrics['transaction_count'] /\n",
    "            (transaction_metrics['customer_age_days'] / 30)\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Calculate days since last purchase\n",
    "        latest_date = pd.to_datetime(transactions['date']).max()\n",
    "        transaction_metrics['days_since_last_purchase'] = (\n",
    "            latest_date - pd.to_datetime(transaction_metrics['last_purchase_date'])\n",
    "        ).dt.days\n",
    "        \n",
    "        return transaction_metrics\n",
    "    \n",
    "# Predicting LTV and Churn\n",
    "\n",
    "    def calculate_customer_ltv(self, \n",
    "                             features: pd.DataFrame, \n",
    "                             months_ahead: int = 12) -> pd.DataFrame:\n",
    "        \"\"\"Calculate customer lifetime value prediction.\"\"\"\n",
    "        \n",
    "        # Calculate historical monthly value\n",
    "        monthly_value = features['total_spend'] / (features['customer_age_days'] / 30)\n",
    "        \n",
    "        # Select features for LTV prediction\n",
    "        ltv_features = [\n",
    "            'age', 'loyalty_score', 'purchase_frequency', 'avg_transaction_value',\n",
    "            'category_diversity', 'premium_product_ratio', 'days_since_last_purchase'\n",
    "        ]\n",
    "        \n",
    "        X = features[ltv_features]\n",
    "        y = monthly_value\n",
    "        \n",
    "        # Split data and train model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        self.ltv_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        monthly_value_pred = self.ltv_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate LTV\n",
    "        ltv_predictions = features.iloc[X_test.index].copy()\n",
    "        ltv_predictions['predicted_monthly_value'] = monthly_value_pred\n",
    "        ltv_predictions['predicted_ltv'] = monthly_value_pred * months_ahead\n",
    "        \n",
    "        # Add feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': ltv_features,\n",
    "            'importance': self.ltv_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return ltv_predictions, feature_importance\n",
    "    \n",
    "    def predict_churn_risk(self, \n",
    "                          features: pd.DataFrame, \n",
    "                          churn_threshold_days: int = 90) -> pd.DataFrame:\n",
    "        \"\"\"Predict customer churn risk.\"\"\"\n",
    "        \n",
    "        # Define churn based on days since last purchase\n",
    "        y = (features['days_since_last_purchase'] > churn_threshold_days).astype(int)\n",
    "        \n",
    "        # Select features for churn prediction\n",
    "        churn_features = [\n",
    "            'age', 'loyalty_score', 'purchase_frequency', 'avg_transaction_value',\n",
    "            'category_diversity', 'premium_product_ratio'\n",
    "        ]\n",
    "        \n",
    "        X = features[churn_features]\n",
    "        \n",
    "        # Split data and train model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        self.churn_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.churn_model.predict(X_test_scaled)\n",
    "        y_pred_proba = self.churn_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = features.iloc[X_test.index].copy()\n",
    "        results['churn_risk'] = y_pred_proba\n",
    "        results['is_churned'] = y_pred\n",
    "        \n",
    "        # Add feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': churn_features,\n",
    "            'importance': self.churn_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        return results, feature_importance\n",
    "\n",
    "# Performing Unsupervised Learning Task: \n",
    "    \n",
    "    def generate_customer_segments(self, \n",
    "                                 features: pd.DataFrame,\n",
    "                                 n_segments: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"Generate value-based customer segments.\"\"\"\n",
    "        \n",
    "        # Calculate RFM scores\n",
    "        recency_score = pd.qcut(\n",
    "            -features['days_since_last_purchase'],\n",
    "            q=5,\n",
    "            labels=['1', '2', '3', '4', '5']\n",
    "        )\n",
    "        \n",
    "        frequency_score = pd.qcut(\n",
    "            features['purchase_frequency'].clip(lower=0),\n",
    "            q=5,\n",
    "            labels=['1', '2', '3', '4', '5']\n",
    "        )\n",
    "        \n",
    "        monetary_score = pd.qcut(\n",
    "            features['total_spend'].clip(lower=0),\n",
    "            q=5,\n",
    "            labels=['1', '2', '3', '4', '5']\n",
    "        )\n",
    "        \n",
    "        # Combine RFM scores\n",
    "        features['rfm_score'] = (\n",
    "            recency_score.astype(str) +\n",
    "            frequency_score.astype(str) +\n",
    "            monetary_score.astype(str)\n",
    "        )\n",
    "        \n",
    "        # Define segment mapping\n",
    "        segment_mapping = {\n",
    "            '555': 'Champions',\n",
    "            '554': 'Champions',\n",
    "            '544': 'Loyal Customers',\n",
    "            '535': 'Loyal Customers',\n",
    "            '444': 'Regular Customers',\n",
    "            '311': 'Lost Customers',\n",
    "            '111': 'Lost Customers'\n",
    "        }\n",
    "        \n",
    "        # Assign segments\n",
    "        features['customer_segment'] = features['rfm_score'].map(\n",
    "            lambda x: segment_mapping.get(x, 'Regular Customers')\n",
    "        )\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have your synthetic data\n",
    "    # generator = BeautyRetailDataGenerator(seed=42)\n",
    "    # customers = generator.generate_customer_profiles(1000)\n",
    "    # transactions = generator.generate_transactions(\n",
    "    #     customers,\n",
    "    #     '2023-01-01',\n",
    "    #     '2024-01-01'\n",
    "    # )\n",
    "    \n",
    "    # Initialize analytics suite\n",
    "    analytics = CustomerAnalyticsSuite()\n",
    "    \n",
    "    # Prepare features\n",
    "    features = analytics.prepare_customer_features(customers, transactions)\n",
    "\n",
    "    # Get LTV predictions\n",
    "    ltv_results, ltv_importance = analytics.calculate_customer_ltv(features)\n",
    "    print(\"\\nLTV Analysis:\")\n",
    "    print(ltv_importance)\n",
    "    \n",
    "    # Get Churn predictions\n",
    "    churn_results, churn_importance = analytics.predict_churn_risk(features)\n",
    "    print(\"\\nChurn Risk Analysis:\")\n",
    "    print(churn_importance)\n",
    "    \n",
    "    # Generate segments\n",
    "    segmented_customers = analytics.generate_customer_segments(features)\n",
    "    print(\"\\nCustomer Segments:\")\n",
    "    print(segmented_customers['customer_segment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd97d69",
   "metadata": {
    "papermill": {
     "duration": 0.002593,
     "end_time": "2025-01-15T23:27:46.259210",
     "exception": false,
     "start_time": "2025-01-15T23:27:46.256617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "RFM is basic compared to more advanced techniques such as K-Means, KNN and Hierarchical Clustering. Will explore this in follow-up notebook."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4551461,
     "sourceId": 10462235,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.036361,
   "end_time": "2025-01-15T23:27:46.981738",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-15T23:27:36.945377",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
